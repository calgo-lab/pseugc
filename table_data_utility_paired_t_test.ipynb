{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bea88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from codealltag_data_processor_v2025 import CodealltagDataProcessor\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from typing import Any, Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d65f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdp_2022 = CodealltagDataProcessor(data_version='20220513', config_path=['codealltag_data_processor_v2025.yml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1fa781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_size_wise_performance_metrics(sample_sizes: List[int],\n",
    "                                             metrics_dir: str,\n",
    "                                             folds: List[int]) -> Tuple[List[List[float]]]:\n",
    "    \n",
    "    precision_list: List[List[float]] = list()\n",
    "    recall_list: List[List[float]] = list()\n",
    "    f1_list: List[List[float]] = list()\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "        sample_precision_list: List[float] = list()\n",
    "        sample_recall_list: List[float] = list()\n",
    "        sample_f1_list: List[float] = list()\n",
    "\n",
    "        for fold in folds:\n",
    "            path = os.path.join(metrics_dir, f'{sample_size}-K{fold}.txt')\n",
    "            with open((Path(path)), \"r\") as file_reader:\n",
    "                sample_fold_dict_str: str = file_reader.read()\n",
    "                sample_fold_dict: Dict = ast.literal_eval(sample_fold_dict_str)\n",
    "                \n",
    "                sample_precision_list.append(sample_fold_dict[\"macro avg\"][\"precision\"])\n",
    "                sample_recall_list.append(sample_fold_dict[\"macro avg\"][\"recall\"])\n",
    "                sample_f1_list.append(sample_fold_dict[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "        precision_list.append(sample_precision_list)\n",
    "        recall_list.append(sample_recall_list)\n",
    "        f1_list.append(sample_f1_list)\n",
    "    \n",
    "    return precision_list, recall_list, f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c929ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_sizes = [3_500]\n",
    "folds = [fold+1 for fold in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf2658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_metrics_dir = os.path.join(*[\"pduc_ner_performance_metrics\", \"orig\", \"bilstm_crf_bpemb_char\"])\n",
    "o_precisions, o_recalls, o_f1_scores = get_sample_size_wise_performance_metrics(sample_sizes, o_metrics_dir, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51ee33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_metrics_dir = os.path.join(*[\"pduc_ner_performance_metrics\", \"pseu\", \"bilstm_crf_bpemb_char\"])\n",
    "p_precisions, p_recalls, p_f1_scores = get_sample_size_wise_performance_metrics(sample_sizes, p_metrics_dir, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f9490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_tuples(precisions: List[List[float]], \n",
    "                        recalls: List[List[float]], \n",
    "                        f1_scores: List[List[float]]) -> List[Tuple]:\n",
    "\n",
    "    output: List[Tuple] = list()\n",
    "    \n",
    "    precision_a = np.array(precisions)\n",
    "    recall_a = np.array(recalls)\n",
    "    f1_score_a = np.array(f1_scores)\n",
    "\n",
    "    precision_means = precision_a.mean(axis=1)\n",
    "    precision_stds = precision_a.std(axis=1)\n",
    "    output.append((precision_means, precision_stds))\n",
    "    \n",
    "    recall_means = recall_a.mean(axis=1)\n",
    "    recall_stds = recall_a.std(axis=1)\n",
    "    output.append((recall_means, recall_stds))\n",
    "    \n",
    "    f1_score_means = f1_score_a.mean(axis=1)\n",
    "    f1_score_stds = f1_score_a.std(axis=1)\n",
    "    output.append((f1_score_means, f1_score_stds))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3c1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mean_std_tuples = get_mean_std_tuples(o_precisions, o_recalls, o_f1_scores)\n",
    "p_mean_std_tuples = get_mean_std_tuples(p_precisions, p_recalls, p_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceee0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_t_stats(group_a: List[float], group_b: List[float]) -> Dict[str, Any]:\n",
    "    t_stat, p_value = stats.ttest_rel(group_a, group_b)\n",
    "    differences = np.array(group_a) - np.array(group_b)\n",
    "    mean_diff = np.mean(differences)\n",
    "    std_diff = np.std(differences, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    n = len(differences)\n",
    "    \n",
    "    # standard error\n",
    "    se_diff = std_diff / np.sqrt(n)\n",
    "    \n",
    "    # degrees of freedom\n",
    "    df = n - 1\n",
    "    \n",
    "    # critical t-value for 95% confidence interval (two-tailed)\n",
    "    t_critical = stats.t.ppf(0.975, df)  # 0.975 for 95% CI (two-tailed)\n",
    "    \n",
    "    # confidence interval\n",
    "    ci_lower = mean_diff - t_critical * se_diff\n",
    "    ci_upper = mean_diff + t_critical * se_diff\n",
    "    \n",
    "    # significance level\n",
    "    significance_level = ''\n",
    "    if p_value < 0.001:\n",
    "        significance_level = '***'\n",
    "    elif p_value < 0.01:\n",
    "        significance_level = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance_level = '*'\n",
    "    \n",
    "    return {\n",
    "        \"p_value\": round(p_value, 4),\n",
    "        \"mean_difference\": round(mean_diff, 4),\n",
    "        \"95_percent_ci\": (round(ci_lower, 4), round(ci_upper, 4)),\n",
    "        \"significance_level\": significance_level\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a78737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_value': 0.0001,\n",
       " 'mean_difference': -0.022,\n",
       " '95_percent_ci': (-0.0292, -0.0148),\n",
       " 'significance_level': '***'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat_precision = calculate_t_stats(o_precisions[0], p_precisions[0]); t_stat_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c18ac1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_value': 0.0397,\n",
       " 'mean_difference': -0.0055,\n",
       " '95_percent_ci': (-0.0107, -0.0003),\n",
       " 'significance_level': '*'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat_recall = calculate_t_stats(o_recalls[0], p_recalls[0]); t_stat_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9879a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_value': 0.0003,\n",
       " 'mean_difference': -0.011,\n",
       " '95_percent_ci': (-0.0154, -0.0066),\n",
       " 'significance_level': '***'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat_f1_score = calculate_t_stats(o_f1_scores[0], p_f1_scores[0]); t_stat_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f96f74f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' 0.92 ± 0.005', ' 0.94 ± 0.007', '0.0001', '***')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_tuple: Tuple = (\n",
    "    f'{round(o_mean_std_tuples[0][0][0], 2): .2f} ± {round(o_mean_std_tuples[0][1][0], 3)}',\n",
    "    f'{round(p_mean_std_tuples[0][0][0], 2): .2f} ± {round(p_mean_std_tuples[0][1][0], 3)}',\n",
    "    f'{t_stat_precision[\"p_value\"]}',\n",
    "    f'{t_stat_precision[\"significance_level\"]}'\n",
    "); precision_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa540a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' 0.88 ± 0.006', ' 0.88 ± 0.006', '0.0397', '*')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_tuple: Tuple = (\n",
    "    f'{round(o_mean_std_tuples[1][0][0], 2): .2f} ± {round(o_mean_std_tuples[1][1][0], 3)}',\n",
    "    f'{round(p_mean_std_tuples[1][0][0], 2): .2f} ± {round(p_mean_std_tuples[1][1][0], 3)}',\n",
    "    f'{t_stat_recall[\"p_value\"]}',\n",
    "    f'{t_stat_recall[\"significance_level\"]}'\n",
    "); recall_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d60fe67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' 0.89 ± 0.004', ' 0.90 ± 0.006', '0.0003', '***')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_tuple: Tuple = (\n",
    "    f'{round(o_mean_std_tuples[2][0][0], 2): .2f} ± {round(o_mean_std_tuples[2][1][0], 3)}',\n",
    "    f'{round(p_mean_std_tuples[2][0][0], 2): .2f} ± {round(p_mean_std_tuples[2][1][0], 3)}',\n",
    "    f'{t_stat_f1_score[\"p_value\"]}',\n",
    "    f'{t_stat_f1_score[\"significance_level\"]}'\n",
    "); f1_score_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff73ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = pd.DataFrame(\n",
    "    [\n",
    "        tuple(['Precision'] + list(precision_tuple)),\n",
    "        tuple(['Recall'] + list(recall_tuple)),\n",
    "        tuple(['F1-score'] + list(f1_score_tuple))\n",
    "    ],\n",
    "    columns=[\"Metric\", \"ORIG | PSEUD\", \"PSEUD | ORIG\", \"p-value\", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "540b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df.set_index(\"Metric\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c4e2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIG | PSEUD</th>\n",
       "      <th>PSEUD | ORIG</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.92 ± 0.005</td>\n",
       "      <td>0.94 ± 0.007</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.88 ± 0.006</td>\n",
       "      <td>0.88 ± 0.006</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.89 ± 0.004</td>\n",
       "      <td>0.90 ± 0.006</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ORIG | PSEUD   PSEUD | ORIG p-value     \n",
       "Metric                                              \n",
       "Precision   0.92 ± 0.005   0.94 ± 0.007  0.0001  ***\n",
       "Recall      0.88 ± 0.006   0.88 ± 0.006  0.0397    *\n",
       "F1-score    0.89 ± 0.004   0.90 ± 0.006  0.0003  ***"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
