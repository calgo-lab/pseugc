{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8934d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 24 18:41:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:87:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             55W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155fd541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s81481/pseugc/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from codealltag_data_processor_v2025 import CodealltagDataProcessor\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from transformers import MT5ForConditionalGeneration, MT5TokenizerFast\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab8c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cb0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(*['/home', 's81481', '.huggingface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f434801",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdp_2022 = CodealltagDataProcessor(data_version='20220513', config_path=['codealltag_data_processor.yml'])\n",
    "cdp_2020 = CodealltagDataProcessor(data_version='20200518', config_path=['codealltag_data_processor.yml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4aaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10_000\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9588f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cdp_2022.get_train_dev_test_datasetdict_for_sample_size(cdp_2020, sample_size, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1aafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9c062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparam):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.hparam = hparam\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(hparam.model_name_or_path, cache_dir=cache_dir)\n",
    "        self.tokenizer = MT5TokenizerFast.from_pretrained(hparam.model_name_or_path, cache_dir=cache_dir)\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                labels=None):\n",
    "        \n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c8d55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mT5/NER-PG/10K/k5/version_0/checkpoints/epoch=02-step=00002-val_loss=1.3786.ckpt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = f\"logs/mT5/NER-PG/{sample_size//1000}K/k{k}/version_0/checkpoints/\"\n",
    "ckpt_name = next(iter(os.listdir(model_dir)), None)\n",
    "model_path = os.path.join(model_dir, ckpt_name); model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8974ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s81481/pseugc/lib/python3.9/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/s81481/pseugc/lib/python3.9/site-packages/transformers/modeling_utils.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "lightning_model = LightningModel.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2237dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lightning_model: LightningModel, input_text: str):\n",
    "    \n",
    "    model = lightning_model.model\n",
    "    tokenizer = lightning_model.tokenizer\n",
    "    \n",
    "    tokenized_outputs = tokenizer.batch_encode_plus(\n",
    "        [input_text],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = tokenized_outputs[\"input_ids\"]\n",
    "    attention_mask = tokenized_outputs[\"attention_mask\"]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          max_length=512,\n",
    "                          temperature=0.8,\n",
    "                          do_sample=True,\n",
    "                          top_k=100)\n",
    "    dec = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "        for ids in outs\n",
    "    ]\n",
    "\n",
    "    return dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicted_text_df(test_df: DataFrame, lightning_model: LightningModel, repeat: int = 5) -> DataFrame:\n",
    "    tuples: List[Tuple] = list()\n",
    "    with tqdm(total=len(test_df), position=0, leave=True) as progress_bar:\n",
    "        for idx, row in test_df.iterrows():\n",
    "            row_items: List[str] = list()\n",
    "            file_path = row.FilePath\n",
    "            row_items.append(file_path)\n",
    "            input_text = cdp_2022.read_email(file_path)[1]\n",
    "            for repeat_num in range(0, repeat):\n",
    "                generated_text = predict(lightning_model=lightning_model, input_text=input_text)\n",
    "                row_items.append(generated_text)\n",
    "            tuples.append(tuple(row_items))\n",
    "            progress_bar.update(1)\n",
    "    return pd.DataFrame(tuples, columns=[\"FilePath\", *[f'V{item+1}' for item in range(0, repeat)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666a2ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [3:22:40<00:00,  6.08s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted_text_df = create_predicted_text_df(test_df=test_df, lightning_model=lightning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eddc7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text_df.to_csv(f'PredictedText_DF_{cdp_2022.get_data_version()}_{sample_size // 1000}K_k{k}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
