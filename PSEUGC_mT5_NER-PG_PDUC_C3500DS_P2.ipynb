{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545bec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import annotations\n",
    "from codealltag_data_processor_v2025 import CodealltagDataProcessor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pandas import DataFrame\n",
    "from pandas.core.series import Series\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, Generator, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a50128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdp_2022 = CodealltagDataProcessor(data_version='20220513', config_path=['codealltag_data_processor.yml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f8263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text_df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(df, index_col=0) \n",
    "        for df in glob.glob(os.path.join('.', f'PredictedText_DF_{cdp_2022.get_data_version()}_15K_*.csv'))\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2fed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['CITY', 'DATE', 'EMAIL', 'FAMILY', 'FEMALE', 'MALE', 'ORG', \n",
    "          'PHONE', 'STREET', 'STREETNO', 'UFID', 'URL', 'USER', 'ZIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccb3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_df_with_input_text_and_predicted_text(input_text: str, \n",
    "                                                         predicted_text: str,\n",
    "                                                         labels: List[str]) -> DataFrame:\n",
    "    tuples = list()\n",
    "\n",
    "    input_text_length = len(input_text)\n",
    "    input_text_copy = input_text[0: input_text_length]\n",
    "\n",
    "    item_delim = \"; \"\n",
    "    token_delim = \": \"\n",
    "    pseudonym_delim = \" **\"\n",
    "    token_id = 0\n",
    "    next_cursor = 0\n",
    "\n",
    "    predicted_items = predicted_text.split(item_delim)\n",
    "    for item in predicted_items:\n",
    "\n",
    "        label, token, pseudonym = \"\", \"\", \"\"\n",
    "\n",
    "        for l in labels:\n",
    "            if item.startswith(l):\n",
    "                label = l\n",
    "\n",
    "        if label != \"\" and (label+token_delim) in item:\n",
    "\n",
    "            value_splits = item.split(label+token_delim)\n",
    "            token_pseudonym = value_splits[1]\n",
    "\n",
    "            if (pseudonym_delim in token_pseudonym and token_pseudonym.endswith(pseudonym_delim.strip())):\n",
    "\n",
    "                pseudonym_splits = token_pseudonym.split(pseudonym_delim)\n",
    "                token = pseudonym_splits[0]\n",
    "                pseudonym = pseudonym_splits[1][:-2]\n",
    "\n",
    "            else:\n",
    "                token = token_pseudonym\n",
    "\n",
    "            if len(token.strip()) > 0:\n",
    "\n",
    "                start = input_text_copy.find(token)\n",
    "                if start == -1 and ' ' in token:\n",
    "                    start = input_text_copy.find(token.split(' ')[0])\n",
    "                    token = token.replace(' ', '')\n",
    "\n",
    "                if start != -1:\n",
    "                    end = start + len(token)\n",
    "\n",
    "                    token_id += 1\n",
    "                    prev_cursor = next_cursor\n",
    "                    next_cursor += end\n",
    "                    input_text_copy = input_text[next_cursor: input_text_length]\n",
    "\n",
    "                    start = prev_cursor + start\n",
    "                    end = prev_cursor + end\n",
    "\n",
    "                    tuples.append((\n",
    "                        'T' + str(token_id),\n",
    "                        label,\n",
    "                        start,\n",
    "                        end,\n",
    "                        input_text[start:end],\n",
    "                        pseudonym\n",
    "                    ))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        tuples,\n",
    "        columns=[\"Token_ID\", \"Label\", \"Start\", \"End\", \"Token\", \"Pseudonym\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfe4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudonymized_text(input_text: str, predicted_annotation_df: DataFrame) -> str:\n",
    "    output_text = input_text\n",
    "    offset = 0\n",
    "    for index, row in predicted_annotation_df.iterrows():\n",
    "        output_text = output_text[:(row.Start+offset)] + row.Pseudonym + output_text[(row.End+offset):]\n",
    "        offset += len(row.Pseudonym) - len(row.Token)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f9ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_df_for_pseudonymized_text(pseudonymized_text: str, pseudonymized_annotation_df: DataFrame) -> DataFrame:\n",
    "    tuples = list()\n",
    "    last_cursor = 0\n",
    "    for index, row in pseudonymized_annotation_df.iterrows():\n",
    "        pseudonym = row.Pseudonym\n",
    "        start = pseudonymized_text[last_cursor:].find(pseudonym)\n",
    "        if start != -1:\n",
    "            tuples.append((row.Token_ID, row.Label, last_cursor+start, last_cursor+start+len(pseudonym), pseudonym))\n",
    "            last_cursor += (start + len(pseudonym))\n",
    "    return pd.DataFrame(\n",
    "        tuples,\n",
    "        columns=[\"Token_ID\", \"Label\", \"Start\", \"End\", \"Token\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dec13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_exact_match_and_prepare_entry_for_new_dataset(predicted_text_df: DataFrame, \n",
    "                                                            idx: int, \n",
    "                                                            cdp: CodealltagDataProcessor) -> Dict[str, Tuple]:\n",
    "    \n",
    "    row: Series = predicted_text_df.iloc[idx]\n",
    "    file_path: str = row.FilePath\n",
    "    original_text: str = cdp.read_email(file_path)[1]\n",
    "    original_adf: DataFrame = cdp.get_annotation_df_by_file(file_path)\n",
    "    original_lt_str: str = '|'.join(original_adf.Label + '-' + original_adf.Token)\n",
    "    versions: List[str] = [col for col in predicted_text_df.columns if re.match(r'^V\\d+$', col)]\n",
    "    version_match_list: List[Tuple] = list()\n",
    "    for version in versions:\n",
    "        version_adf: DataFrame = get_annotation_df_with_input_text_and_predicted_text(original_text, row[version], labels)\n",
    "        version_lt_str: str = '|'.join(version_adf.Label + '-' + version_adf.Token)\n",
    "        if original_lt_str == version_lt_str and (version_adf.Token != version_adf.Pseudonym).all():\n",
    "            version_match_list.append((version, row[version], version_adf))\n",
    "    if len(version_match_list) == 0:\n",
    "        return {file_path: None}\n",
    "    version_tuple = None\n",
    "    if len(version_match_list) == 1:\n",
    "        version_tuple = version_match_list[0]\n",
    "    else:\n",
    "        version_tuple = random.Random(cdp.get_random_seed()).choice(version_match_list)\n",
    "    original_tokenized_text = \" \".join(cdp.tokenize_with_somajo(original_text))\n",
    "    original_tokenized_text_with_label = cdp.tokenize_with_somajo_and_annotation_df(original_text, original_adf)\n",
    "    mT5_output = version_tuple[1]\n",
    "    mT5_adf = version_tuple[2]\n",
    "    mT5_pseudonymized_text = get_pseudonymized_text(original_text, mT5_adf)\n",
    "    mT5_pseudonymized_tokenized_text = \" \".join(cdp.tokenize_with_somajo(mT5_pseudonymized_text))\n",
    "    mT5_s_adf = get_annotation_df_for_pseudonymized_text(mT5_pseudonymized_text, mT5_adf)\n",
    "    mT5_pseudonymized_tokenized_text_with_label = cdp.tokenize_with_somajo_and_annotation_df(mT5_pseudonymized_text, mT5_s_adf)\n",
    "    output_tuple = (\n",
    "        file_path,\n",
    "        original_text,\n",
    "        original_adf.drop(columns=\"FilePath\").to_dict(),\n",
    "        original_tokenized_text,\n",
    "        original_tokenized_text_with_label,\n",
    "        mT5_output,\n",
    "        mT5_pseudonymized_text,\n",
    "        mT5_adf.to_dict(),\n",
    "        mT5_pseudonymized_tokenized_text,\n",
    "        mT5_pseudonymized_tokenized_text_with_label\n",
    "    )\n",
    "    return {file_path: output_tuple}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aafca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_entries_for_new_dataset(max_workers: int = 10) -> Generator[Dict[str, Tuple]]:\n",
    "    with tqdm(total=len(predicted_text_df), smoothing=0) as progress_bar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [\n",
    "                executor.submit(check_for_exact_match_and_prepare_entry_for_new_dataset, predicted_text_df, idx, cdp_2022)\n",
    "                for idx in range(0, len(predicted_text_df))\n",
    "            ]\n",
    "            for future in as_completed(futures):\n",
    "                progress_bar.update(1)\n",
    "                yield future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28764be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 15000/15000 [2:47:31<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_dict: Dict[str, Tuple] = {}\n",
    "for result in collect_entries_for_new_dataset():\n",
    "    merged_dict.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0e9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [merged_dict[fp] for fp in merged_dict.keys() if merged_dict[fp]]\n",
    "columns = [\"FilePath\", \"OT\", \"OA\", \"OTT\", \"OTTL\", \"MT5O\", \"MT5PT\", \"MT5PA\", \"MT5PTT\", \"MT5PTTL\"]\n",
    "dataset_df = pd.DataFrame(tuples, columns)\n",
    "dataset_df.to_csv(\"data_utility_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d11b2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data_utility_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef20c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f6de29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Siegfried T.  (whoqob) schrieb:\n",
      "\n",
      "[...]\n",
      "\n",
      "im Grunde genommen interessiert es mich nicht, ich schau auf den Chart  \n",
      ":-)\n",
      "\n",
      "Muß allerdings eingestehen, das ich, aufgeheizt durch diese Group einig  \n",
      "zig davon, zum Glück verkehrt geodert und somit nicht bekommen habe.\n",
      "\n",
      "Keine Angst.... dafür habe ich in eine andere Kloschüssel gegriffen...\n",
      "\n",
      "\n",
      "  Gruss  Benjamin\n",
      "\n",
      "PS. Nun warte ich auf den Klempner.... und dann wird es schon wieder im  \n",
      "Abfluß brodeln ;-)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].OT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0ede607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>MALE</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>Siegfried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>T.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>USER</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>whoqob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>MALE</td>\n",
       "      <td>347</td>\n",
       "      <td>355</td>\n",
       "      <td>Benjamin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token_ID   Label Start  End      Token\n",
       "0       T1    MALE     3   12  Siegfried\n",
       "1       T2  FAMILY    13   15         T.\n",
       "2       T3    USER    18   24     whoqob\n",
       "3       T4    MALE   347  355   Benjamin"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ast.literal_eval(dataset.iloc[idx].OA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23067aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * Siegfried T. ( whoqob ) schrieb : [ ... ] im Grunde genommen interessiert es mich nicht , ich schau auf den Chart :-) Muß allerdings eingestehen , das ich , aufgeheizt durch diese Group einig zig davon , zum Glück verkehrt geodert und somit nicht bekommen habe . Keine Angst .... dafür habe ich in eine andere Kloschüssel gegriffen ... Gruss Benjamin PS. Nun warte ich auf den Klempner .... und dann wird es schon wieder im Abfluß brodeln ;-)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].OTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6c7eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* O\n",
      "* O\n",
      "* O\n",
      "Siegfried B-MALE\n",
      "T. B-FAMILY\n",
      "( O\n",
      "whoqob B-USER\n",
      ") O\n",
      "schrieb O\n",
      ": O\n",
      "[ O\n",
      "... O\n",
      "] O\n",
      "im O\n",
      "Grunde O\n",
      "genommen O\n",
      "interessiert O\n",
      "es O\n",
      "mich O\n",
      "nicht O\n",
      ", O\n",
      "ich O\n",
      "schau O\n",
      "auf O\n",
      "den O\n",
      "Chart O\n",
      ":-) O\n",
      "Muß O\n",
      "allerdings O\n",
      "eingestehen O\n",
      ", O\n",
      "das O\n",
      "ich O\n",
      ", O\n",
      "aufgeheizt O\n",
      "durch O\n",
      "diese O\n",
      "Group O\n",
      "einig O\n",
      "zig O\n",
      "davon O\n",
      ", O\n",
      "zum O\n",
      "Glück O\n",
      "verkehrt O\n",
      "geodert O\n",
      "und O\n",
      "somit O\n",
      "nicht O\n",
      "bekommen O\n",
      "habe O\n",
      ". O\n",
      "Keine O\n",
      "Angst O\n",
      ".... O\n",
      "dafür O\n",
      "habe O\n",
      "ich O\n",
      "in O\n",
      "eine O\n",
      "andere O\n",
      "Kloschüssel O\n",
      "gegriffen O\n",
      "... O\n",
      "Gruss O\n",
      "Benjamin B-MALE\n",
      "PS. O\n",
      "Nun O\n",
      "warte O\n",
      "ich O\n",
      "auf O\n",
      "den O\n",
      "Klempner O\n",
      ".... O\n",
      "und O\n",
      "dann O\n",
      "wird O\n",
      "es O\n",
      "schon O\n",
      "wieder O\n",
      "im O\n",
      "Abfluß O\n",
      "brodeln O\n",
      ";-) O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].OTTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c350922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MALE: Siegfried **Ottmar**; FAMILY: T. **W.**; USER: whoqob **knkqjm**; MALE: Benjamin **Vinzenz**\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].MT5O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b725df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Ottmar W.  (knkqjm) schrieb:\n",
      "\n",
      "[...]\n",
      "\n",
      "im Grunde genommen interessiert es mich nicht, ich schau auf den Chart  \n",
      ":-)\n",
      "\n",
      "Muß allerdings eingestehen, das ich, aufgeheizt durch diese Group einig  \n",
      "zig davon, zum Glück verkehrt geodert und somit nicht bekommen habe.\n",
      "\n",
      "Keine Angst.... dafür habe ich in eine andere Kloschüssel gegriffen...\n",
      "\n",
      "\n",
      "  Gruss  Vinzenz\n",
      "\n",
      "PS. Nun warte ich auf den Klempner.... und dann wird es schon wieder im  \n",
      "Abfluß brodeln ;-)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].MT5PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b8676e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Token</th>\n",
       "      <th>Pseudonym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>MALE</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>Siegfried</td>\n",
       "      <td>Ottmar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>T.</td>\n",
       "      <td>W.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>USER</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>whoqob</td>\n",
       "      <td>knkqjm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>MALE</td>\n",
       "      <td>347</td>\n",
       "      <td>355</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>Vinzenz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token_ID   Label  Start  End      Token Pseudonym\n",
       "0       T1    MALE      3   12  Siegfried    Ottmar\n",
       "1       T2  FAMILY     13   15         T.        W.\n",
       "2       T3    USER     18   24     whoqob    knkqjm\n",
       "3       T4    MALE    347  355   Benjamin   Vinzenz"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ast.literal_eval(dataset.iloc[idx].MT5PA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f794a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * Ottmar W. ( knkqjm ) schrieb : [ ... ] im Grunde genommen interessiert es mich nicht , ich schau auf den Chart :-) Muß allerdings eingestehen , das ich , aufgeheizt durch diese Group einig zig davon , zum Glück verkehrt geodert und somit nicht bekommen habe . Keine Angst .... dafür habe ich in eine andere Kloschüssel gegriffen ... Gruss Vinzenz PS. Nun warte ich auf den Klempner .... und dann wird es schon wieder im Abfluß brodeln ;-)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].MT5PTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83296a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* O\n",
      "* O\n",
      "* O\n",
      "Ottmar B-MALE\n",
      "W. B-FAMILY\n",
      "( O\n",
      "knkqjm B-USER\n",
      ") O\n",
      "schrieb O\n",
      ": O\n",
      "[ O\n",
      "... O\n",
      "] O\n",
      "im O\n",
      "Grunde O\n",
      "genommen O\n",
      "interessiert O\n",
      "es O\n",
      "mich O\n",
      "nicht O\n",
      ", O\n",
      "ich O\n",
      "schau O\n",
      "auf O\n",
      "den O\n",
      "Chart O\n",
      ":-) O\n",
      "Muß O\n",
      "allerdings O\n",
      "eingestehen O\n",
      ", O\n",
      "das O\n",
      "ich O\n",
      ", O\n",
      "aufgeheizt O\n",
      "durch O\n",
      "diese O\n",
      "Group O\n",
      "einig O\n",
      "zig O\n",
      "davon O\n",
      ", O\n",
      "zum O\n",
      "Glück O\n",
      "verkehrt O\n",
      "geodert O\n",
      "und O\n",
      "somit O\n",
      "nicht O\n",
      "bekommen O\n",
      "habe O\n",
      ". O\n",
      "Keine O\n",
      "Angst O\n",
      ".... O\n",
      "dafür O\n",
      "habe O\n",
      "ich O\n",
      "in O\n",
      "eine O\n",
      "andere O\n",
      "Kloschüssel O\n",
      "gegriffen O\n",
      "... O\n",
      "Gruss O\n",
      "Vinzenz B-MALE\n",
      "PS. O\n",
      "Nun O\n",
      "warte O\n",
      "ich O\n",
      "auf O\n",
      "den O\n",
      "Klempner O\n",
      ".... O\n",
      "und O\n",
      "dann O\n",
      "wird O\n",
      "es O\n",
      "schon O\n",
      "wieder O\n",
      "im O\n",
      "Abfluß O\n",
      "brodeln O\n",
      ";-) O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[idx].MT5PTTL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
