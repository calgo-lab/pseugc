{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8934d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 10 21:40:35 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:47:00.0 Off |                    0 |\r\n",
      "| N/A   31C    P0             63W /  400W |       1MiB /  40960MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155fd541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s81481/pseugc/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from pandas import DataFrame\n",
    "from timeit import default_timer as timer\n",
    "from transformers import MT5ForConditionalGeneration, MT5TokenizerFast\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab8c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9c062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparam):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.hparam = hparam\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(hparam.model_name_or_path)\n",
    "        self.tokenizer = MT5TokenizerFast.from_pretrained(hparam.model_name_or_path)\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                labels=None):\n",
    "        \n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8974ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s81481/pseugc/lib/python3.9/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/s81481/pseugc/lib/python3.9/site-packages/transformers/modeling_utils.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_path = \"logs/mT5/NER-PG/10K/k3/version_0/checkpoints/epoch=03-step=00003-val_loss=1.3927.ckpt\"\n",
    "lightning_model = LightningModel.load_from_checkpoint(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2237dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(lightning_model: LightningModel, input_text: str):\n",
    "        \n",
    "    start = timer()\n",
    "    \n",
    "    model = lightning_model.model\n",
    "    tokenizer = lightning_model.tokenizer\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_stat = \"CPU\" if device == \"cpu\" else torch.cuda.get_device_name(0)\n",
    "    print(f\"device_stat: {device_stat}\")\n",
    "\n",
    "    tokenized_outputs = tokenizer.batch_encode_plus(\n",
    "        [input_text],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = tokenized_outputs[\"input_ids\"]\n",
    "    attention_mask = tokenized_outputs[\"attention_mask\"]\n",
    "\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          max_length=512,\n",
    "                          temperature=0.8,\n",
    "                          do_sample=True,\n",
    "                          top_k=100)\n",
    "    dec = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "        for ids in outs\n",
    "    ]\n",
    "\n",
    "    end = timer()\n",
    "\n",
    "    print(f\"inference_time: {round(end - start, 3)}s\")\n",
    "\n",
    "    return dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5819b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_df_with_input_text_and_predicted_text(input_text: str, \n",
    "                                                         predicted_text: str,\n",
    "                                                         labels: List[str]) -> DataFrame:\n",
    "    tuples = list()\n",
    "\n",
    "    input_text_length = len(input_text)\n",
    "    input_text_copy = input_text[0: input_text_length]\n",
    "\n",
    "    item_delim = \"; \"\n",
    "    token_delim = \": \"\n",
    "    pseudonym_delim = \" **\"\n",
    "    token_id = 0\n",
    "    next_cursor = 0\n",
    "\n",
    "    predicted_items = predicted_text.split(item_delim)\n",
    "    for item in predicted_items:\n",
    "\n",
    "        label, token, pseudonym = \"\", \"\", \"\"\n",
    "\n",
    "        for l in labels:\n",
    "            if item.startswith(l):\n",
    "                label = l\n",
    "\n",
    "        if label != \"\" and (label+token_delim) in item:\n",
    "\n",
    "            value_splits = item.split(label+token_delim)\n",
    "            token_pseudonym = value_splits[1]\n",
    "\n",
    "            if (pseudonym_delim in token_pseudonym and token_pseudonym.endswith(pseudonym_delim.strip())):\n",
    "\n",
    "                pseudonym_splits = token_pseudonym.split(pseudonym_delim)\n",
    "                token = pseudonym_splits[0]\n",
    "                pseudonym = pseudonym_splits[1][:-2]\n",
    "\n",
    "            else:\n",
    "                token = token_pseudonym\n",
    "\n",
    "            if len(token.strip()) > 0:\n",
    "\n",
    "                start = input_text_copy.find(token)\n",
    "                if start == -1 and ' ' in token:\n",
    "                    start = input_text_copy.find(token.split(' ')[0])\n",
    "                    token = token.replace(' ', '')\n",
    "\n",
    "                if start != -1:\n",
    "                    end = start + len(token)\n",
    "\n",
    "                    token_id += 1\n",
    "                    prev_cursor = next_cursor\n",
    "                    next_cursor += end\n",
    "                    input_text_copy = input_text[next_cursor: input_text_length]\n",
    "\n",
    "                    start = prev_cursor + start\n",
    "                    end = prev_cursor + end\n",
    "\n",
    "                    tuples.append((\n",
    "                        'T' + str(token_id),\n",
    "                        label,\n",
    "                        start,\n",
    "                        end,\n",
    "                        input_text[start:end],\n",
    "                        pseudonym\n",
    "                    ))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        tuples,\n",
    "        columns=[\"Token_ID\", \"Label\", \"Start\", \"End\", \"Token\", \"Pseudonym\"]\n",
    "    )\n",
    "\n",
    "def get_pseudonymized_text(input_text: str, predicted_annotation_df: DataFrame) -> str:\n",
    "    output_text = input_text\n",
    "    offset = 0\n",
    "    for index, row in predicted_annotation_df.iterrows():\n",
    "        output_text = output_text[:(row.Start+offset)] + row.Pseudonym + output_text[(row.End+offset):]\n",
    "        offset += len(row.Pseudonym) - len(row.Token)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e14574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sample_text(lightning_model: LightningModel, input_text: str) -> str:\n",
    "    labels = ['CITY', 'DATE', 'EMAIL', 'FAMILY', 'FEMALE', 'MALE', 'ORG', 'PHONE', 'STREET', 'STREETNO', 'UFID', 'URL', 'USER', 'ZIP']\n",
    "    predicted_text = predict(lightning_model, input_text)\n",
    "    print(f\"predicted_text: {predicted_text}\")\n",
    "    output_df = get_annotation_df_with_input_text_and_predicted_text(input_text, predicted_text, labels)\n",
    "    output_text = get_pseudonymized_text(input_text, output_df)\n",
    "    return output_df, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22fbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "Am Sun, 21 Nov 2013 20:50:06 +0100 schrieb Mr. TIMO:\n",
    "\n",
    "\n",
    "Man passt sich im allgemeinen erst mal den üblichen Gepflogenheiten an wenn\n",
    "man irgendwo neu dazu kommt.\n",
    "\n",
    "Wenn das Signieren üblich wäre würden es die verbreiteten Newsreader in der\n",
    "Darstellung filtern. Es nervt beim lesen jedes Postings.\n",
    "\n",
    "Vincenzo\n",
    "\n",
    "-- \n",
    "Mit unseren Sensoren ist der Administrator informiert, bevor es Probleme im \n",
    "Serverraum gibt: preiswerte Monitoring Hard- und Software-kostenloses Plugin \n",
    "auch für Nagios - Nachricht per e-mail,SMS und SNMP: http://qpr.azkmja.ye\n",
    "Messwerte nachträgliche Wärmedämmung http://sch.zwkapb.af/jhnhjtewbyqgyh.rzh\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8798d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_stat: NVIDIA A100-SXM4-40GB\n",
      "inference_time: 1.737s\n",
      "predicted_text: DATE: 21 Nov 2013 **07 Okt 2013**; MALE: TIMO **Heino**; MALE: Vincenzo **Italo**; ORG: Nagios **FDV**; URL: http://qpr.azkmja.ye **http://qgy.xmhbzq.nk**; URL: http://sch.zwkapb.af/jhnhjtewbyqgyh.rzh **http://vl.xkhdpt.qu/hjdtdtjspnvgjp.dxi**\n"
     ]
    }
   ],
   "source": [
    "output_df, output_text = predict_for_sample_text(lightning_model, input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "547ae2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Token</th>\n",
       "      <th>Pseudonym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>DATE</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>21 Nov 2013</td>\n",
       "      <td>07 Okt 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>MALE</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>TIMO</td>\n",
       "      <td>Heino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>MALE</td>\n",
       "      <td>296</td>\n",
       "      <td>304</td>\n",
       "      <td>Vincenzo</td>\n",
       "      <td>Italo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>ORG</td>\n",
       "      <td>474</td>\n",
       "      <td>480</td>\n",
       "      <td>Nagios</td>\n",
       "      <td>FDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>URL</td>\n",
       "      <td>518</td>\n",
       "      <td>538</td>\n",
       "      <td>http://qpr.azkmja.ye</td>\n",
       "      <td>http://qgy.xmhbzq.nk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T6</td>\n",
       "      <td>URL</td>\n",
       "      <td>576</td>\n",
       "      <td>615</td>\n",
       "      <td>http://sch.zwkapb.af/jhnhjtewbyqgyh.rzh</td>\n",
       "      <td>http://vl.xkhdpt.qu/hjdtdtjspnvgjp.dxi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token_ID Label  Start  End                                    Token  \\\n",
       "0       T1  DATE      9   20                              21 Nov 2013   \n",
       "1       T2  MALE     48   52                                     TIMO   \n",
       "2       T3  MALE    296  304                                 Vincenzo   \n",
       "3       T4   ORG    474  480                                   Nagios   \n",
       "4       T5   URL    518  538                     http://qpr.azkmja.ye   \n",
       "5       T6   URL    576  615  http://sch.zwkapb.af/jhnhjtewbyqgyh.rzh   \n",
       "\n",
       "                                Pseudonym  \n",
       "0                             07 Okt 2013  \n",
       "1                                   Heino  \n",
       "2                                   Italo  \n",
       "3                                     FDV  \n",
       "4                    http://qgy.xmhbzq.nk  \n",
       "5  http://vl.xkhdpt.qu/hjdtdtjspnvgjp.dxi  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1ae6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Am Sun, 07 Okt 2013 20:50:06 +0100 schrieb Mr. Heino:\n",
      "\n",
      "\n",
      "Man passt sich im allgemeinen erst mal den üblichen Gepflogenheiten an wenn\n",
      "man irgendwo neu dazu kommt.\n",
      "\n",
      "Wenn das Signieren üblich wäre würden es die verbreiteten Newsreader in der\n",
      "Darstellung filtern. Es nervt beim lesen jedes Postings.\n",
      "\n",
      "Italo\n",
      "\n",
      "-- \n",
      "Mit unseren Sensoren ist der Administrator informiert, bevor es Probleme im \n",
      "Serverraum gibt: preiswerte Monitoring Hard- und Software-kostenloses Plugin \n",
      "auch für FDV - Nachricht per e-mail,SMS und SNMP: http://qgy.xmhbzq.nk\n",
      "Messwerte nachträgliche Wärmedämmung http://vl.xkhdpt.qu/hjdtdtjspnvgjp.dxi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
