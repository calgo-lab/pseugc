{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984d916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288ea740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_size_wise_performance_metrics(sample_sizes: List[int],\n",
    "                                             metrics_dir: str) -> Tuple[List[List[float]]]:\n",
    "    \n",
    "    precision_list: List[List[float]] = list()\n",
    "    recall_list: List[List[float]] = list()\n",
    "    f1_list: List[List[float]] = list()\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "        sample_precision_list: List[float] = list()\n",
    "        sample_recall_list: List[float] = list()\n",
    "        sample_f1_list: List[float] = list()\n",
    "\n",
    "        for item in glob.glob(os.path.join(metrics_dir, str(sample_size//1000)+\"*.txt\")):\n",
    "            with open((Path(item)), \"r\") as file_reader:\n",
    "                sample_fold_dict_str: str = file_reader.read()\n",
    "                sample_fold_dict: Dict = ast.literal_eval(sample_fold_dict_str)\n",
    "                \n",
    "                sample_precision_list.append(sample_fold_dict[\"macro avg\"][\"precision\"])\n",
    "                sample_recall_list.append(sample_fold_dict[\"macro avg\"][\"recall\"])\n",
    "                sample_f1_list.append(sample_fold_dict[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "        precision_list.append(sample_precision_list)\n",
    "        recall_list.append(sample_recall_list)\n",
    "        f1_list.append(sample_f1_list)\n",
    "    \n",
    "    return precision_list, recall_list, f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe92c883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Reall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NER</td>\n",
       "      <td>0.93 ± 0.004</td>\n",
       "      <td>0.88 ± 0.035</td>\n",
       "      <td>0.90 ± 0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PG (Type Compliance)</td>\n",
       "      <td>0.87 ± 0.005</td>\n",
       "      <td>0.82 ± 0.033</td>\n",
       "      <td>0.84 ± 0.021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Component      Precision          Reall       F1-score\n",
       "0                   NER   0.93 ± 0.004   0.88 ± 0.035   0.90 ± 0.022\n",
       "1  PG (Type Compliance)   0.87 ± 0.005   0.82 ± 0.033   0.84 ± 0.021"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sizes = [10_000]\n",
    "\n",
    "ner_metrics_dir = os.path.join(*[\"ner_performance_metrics\", \"mt5\"])\n",
    "ner_precision, ner_recall, ner_f1_score = get_sample_size_wise_performance_metrics(sample_sizes, ner_metrics_dir)\n",
    "\n",
    "ner_precision = np.array(ner_precision)\n",
    "ner_recall = np.array(ner_recall)\n",
    "ner_f1_score = np.array(ner_f1_score)\n",
    "\n",
    "# Mean and standard deviation\n",
    "ner_precision_mean = ner_precision.mean(axis=1)\n",
    "ner_precision_std = ner_precision.std(axis=1)\n",
    "ner_recall_mean = ner_recall.mean(axis=1)\n",
    "ner_recall_std = ner_recall.std(axis=1)\n",
    "ner_f1_score_mean = ner_f1_score.mean(axis=1)\n",
    "ner_f1_score_std = ner_f1_score.std(axis=1)\n",
    "\n",
    "ner_tuple: Tuple[str] = (\n",
    "    \"NER\",\n",
    "    f'{round(ner_precision_mean[0], 2): .2f} ± {round(ner_precision_std[0], 3)}',\n",
    "    f'{round(ner_recall_mean[0], 2): .2f} ± {round(ner_recall_std[0], 3)}',\n",
    "    f'{round(ner_f1_score_mean[0], 2): .2f} ± {round(ner_f1_score_std[0], 3)}',\n",
    ")\n",
    "\n",
    "pg_tc_metrics_dir = os.path.join(*[\"pseudonym_generation_type_compliance_metrics\", \"mt5\"])\n",
    "pg_tc_precision, pg_tc_recall, pg_tc_f1_score = get_sample_size_wise_performance_metrics(sample_sizes, pg_tc_metrics_dir)\n",
    "\n",
    "pg_tc_precision = np.array(pg_tc_precision)\n",
    "pg_tc_recall = np.array(pg_tc_recall)\n",
    "pg_tc_f1_score = np.array(pg_tc_f1_score)\n",
    "\n",
    "# Mean and standard deviation\n",
    "pg_tc_precision_mean = pg_tc_precision.mean(axis=1)\n",
    "pg_tc_precision_std = pg_tc_precision.std(axis=1)\n",
    "pg_tc_recall_mean = pg_tc_recall.mean(axis=1)\n",
    "pg_tc_recall_std = pg_tc_recall.std(axis=1)\n",
    "pg_tc_f1_score_mean = pg_tc_f1_score.mean(axis=1)\n",
    "pg_tc_f1_score_std = pg_tc_f1_score.std(axis=1)\n",
    "\n",
    "pg_tc_tuple: Tuple[str] = (\n",
    "    \"PG (Type Compliance)\",\n",
    "    f'{round(pg_tc_precision_mean[0], 2): .2f} ± {round(pg_tc_precision_std[0], 3)}',\n",
    "    f'{round(pg_tc_recall_mean[0], 2): .2f} ± {round(pg_tc_recall_std[0], 3)}',\n",
    "    f'{round(pg_tc_f1_score_mean[0], 2): .2f} ± {round(pg_tc_f1_score_std[0], 3)}',\n",
    ")\n",
    "\n",
    "pd.DataFrame([ner_tuple, pg_tc_tuple], columns=[\"Component\", \"Precision\", \"Reall\", \"F1-score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
